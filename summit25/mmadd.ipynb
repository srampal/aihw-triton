{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579dde73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMatrix Multiplication\\n=====================\\nIn this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\\nperformance on par with cuBLAS or rocBLAS.\\n\\nYou will specifically learn about:\\n\\n* Block-level matrix multiplications.\\n\\n* Multi-dimensional pointer arithmetic.\\n\\n* Program re-ordering for improved L2 cache hit rate.\\n\\n* Automatic performance tuning.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Matrix Multiplication\n",
    "=====================\n",
    "In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n",
    "performance on par with cuBLAS or rocBLAS.\n",
    "\n",
    "You will specifically learn about:\n",
    "\n",
    "* Block-level matrix multiplications.\n",
    "\n",
    "* Multi-dimensional pointer arithmetic.\n",
    "\n",
    "* Program re-ordering for improved L2 cache hit rate.\n",
    "\n",
    "* Automatic performance tuning.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9578b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motivations\n",
    "# -----------\n",
    "#\n",
    "# Matrix multiplications are a key building block of most modern high-performance computing systems.\n",
    "# They are notoriously hard to optimize, hence their implementation is generally done by\n",
    "# hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n",
    "# Unfortunately, these libraries are often proprietary and cannot be easily customized\n",
    "# to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n",
    "# In this tutorial, you will learn how to implement efficient matrix multiplications by\n",
    "# yourself with Triton, in a way that is easy to customize and extend.\n",
    "#\n",
    "# Roughly speaking, the kernel that we will write will implement the following blocked\n",
    "# algorithm to multiply a (M, K) by a (K, N) matrix:\n",
    "#\n",
    "#  .. code-block:: python\n",
    "#\n",
    "#    # Do in parallel\n",
    "#    for m in range(0, M, BLOCK_SIZE_M):\n",
    "#      # Do in parallel\n",
    "#      for n in range(0, N, BLOCK_SIZE_N):\n",
    "#        acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\n",
    "#        for k in range(0, K, BLOCK_SIZE_K):\n",
    "#          a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n",
    "#          x = X[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n",
    "#          acc += dot(a, x)\n",
    "#        C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\n",
    "#\n",
    "# where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce83832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Kernel\n",
    "# --------------\n",
    "#\n",
    "# The above algorithm is, actually, fairly straightforward to implement in Triton.\n",
    "# The main difficulty comes from the computation of the memory locations at which blocks\n",
    "# of :code:`A` and :code:`X` must be read in the inner loop. For that, we need\n",
    "# multi-dimensional pointer arithmetic.\n",
    "#\n",
    "# Pointer Arithmetic\n",
    "# ~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given\n",
    "# by :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n",
    "# Therefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\n",
    "# :code:`X[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\n",
    "#\n",
    "#  .. code-block:: python\n",
    "#\n",
    "#    &A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\n",
    "#    &X[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  x_ptr + (k : k+BLOCK_SIZE_K)[:, None]*X.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*X.stride(1);\n",
    "#\n",
    "# Which means that pointers for blocks of A and X can be initialized (i.e., :code:`k=0`) in Triton as the following\n",
    "# code. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\n",
    "# :code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\n",
    "# some useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\n",
    "# using masking load semantics.\n",
    "#\n",
    "#  .. code-block:: python\n",
    "#\n",
    "#    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "#    offs_xn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "#    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "#    a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\n",
    "#    x_ptrs = x_ptr + (offs_k [:, None]*stride_xk + offs_xn[None, :]*stride_xn)\n",
    "#\n",
    "# And then updated in the inner loop as follows:\n",
    "#\n",
    "#  .. code-block:: python\n",
    "#\n",
    "#    a_ptrs += BLOCK_SIZE_K * stride_ak;\n",
    "#    x_ptrs += BLOCK_SIZE_K * stride_xk;\n",
    "#\n",
    "#\n",
    "# L2 Cache Optimizations\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# As mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\n",
    "# block of :code:`C`.\n",
    "# It is important to remember that the order in which these blocks are computed does\n",
    "# matter, since it affects the L2 cache hit rate of our program, and unfortunately, a\n",
    "# simple row-major ordering\n",
    "#\n",
    "#  .. code-block:: Python\n",
    "#\n",
    "#    pid = tl.program_id(axis=0)\n",
    "#    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "#    pid_m = pid // grid_n\n",
    "#    pid_n = pid % grid_n\n",
    "#\n",
    "# is just not going to cut it.\n",
    "#\n",
    "# One possible solution is to launch blocks in an order that promotes data reuse.\n",
    "# This can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\n",
    "# switching to the next column:\n",
    "#\n",
    "#  .. code-block:: python\n",
    "#\n",
    "#    # Program ID\n",
    "#    pid = tl.program_id(axis=0)\n",
    "#    # Number of program ids along the M axis\n",
    "#    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "#    # Number of programs ids along the N axis\n",
    "#    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "#    # Number of programs in group\n",
    "#    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "#    # Id of the group this program is in\n",
    "#    group_id = pid // num_pid_in_group\n",
    "#    # Row-id of the first program in the group\n",
    "#    first_pid_m = group_id * GROUP_SIZE_M\n",
    "#    # If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n",
    "#    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "#    # *Within groups*, programs are ordered in a column-major order\n",
    "#    # Row-id of the program in the *launch grid*\n",
    "#    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "#    # Col-id of the program in the *launch grid*\n",
    "#    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "#\n",
    "# For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n",
    "# we can see that if we compute the output in row-major ordering, we need to load 90\n",
    "# blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n",
    "# ordering, we only need to load 54 blocks.\n",
    "#\n",
    "#   .. image:: grouped_vs_row_major_ordering.png\n",
    "#\n",
    "# In practice, this can improve the performance of our matrix multiplication kernel by\n",
    "# more than 10\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d387427e-fb2b-42de-99ca-d401c4ab0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build your own triton\n",
    "#%env MAX_JOBS=6\n",
    "#!cd triton && pip install ./python && cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8bd9bd6-2abc-4727-9b6b-924e73cba314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: triton\n",
      "Version: 3.3.0\n",
      "Summary: A language and compiler for custom Deep Learning operations\n",
      "Home-page: https://github.com/triton-lang/triton/\n",
      "Author: Philippe Tillet\n",
      "Author-email: phil@openai.com\n",
      "License: \n",
      "Location: /workspace/lib/python3.12/site-packages\n",
      "Requires: setuptools\n",
      "Required-by: torch\n",
      "\u001b[33mWARNING: Package(s) not found: pytorch-triton\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip show triton\n",
    "!pip show pytorch-triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63abf347",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TORCH_LOGS\"] = \"output_code\"\n",
    "\n",
    "# Final Result\n",
    "# ------------\n",
    "\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# triton 3.2\n",
    "#DEVICE = triton.runtime.driver.active.get_current_target().backend\n",
    "# triton 3.3\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "\n",
    "\n",
    "def is_hip_mi200():\n",
    "    target = triton.runtime.driver.active.get_current_target()\n",
    "    return target.backend == 'hip' and target.arch == 'gfx90a'\n",
    "\n",
    "\n",
    "def get_cuda_autotune_config():\n",
    "    return [\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        # Good config for fp8 inputs.\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_hip_autotune_config():\n",
    "    return [\n",
    "        triton.Config(\n",
    "            {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2},\n",
    "            num_warps=4, num_stages=2),\n",
    "        triton.Config(\n",
    "            {'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 4, 'waves_per_eu': 2},\n",
    "            num_warps=8, num_stages=2),\n",
    "        triton.Config(\n",
    "            {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2},\n",
    "            num_warps=8, num_stages=2),\n",
    "        triton.Config(\n",
    "            {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8, 'waves_per_eu': 3},\n",
    "            num_warps=4, num_stages=2),\n",
    "        triton.Config(\n",
    "            {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 1, 'waves_per_eu': 8},\n",
    "            num_warps=4, num_stages=2),\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_autotune_config():\n",
    "    if is_cuda():\n",
    "        return get_cuda_autotune_config()\n",
    "    else:\n",
    "        return get_hip_autotune_config()\n",
    "\n",
    "\n",
    "# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n",
    "#   - A list of `triton.Config` objects that define different configurations of\n",
    "#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n",
    "#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n",
    "#       provided configs\n",
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_add_kernel(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, x_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_xk, stride_xn,  #\n",
    "        stride_cm, stride_cn,  # matrix b has same stride as c\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmuladd C = (A x X) + B.\n",
    "    A has shape (M, K), X has shape (K, N) and B and C have shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and X.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `x_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetic` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_xn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    x_ptrs = x_ptr + (offs_k[:, None] * stride_xk + offs_xn[None, :] * stride_xn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and X, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        x = tl.load(x_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, x, accumulator)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        x_ptrs += BLOCK_SIZE_K * stride_xk\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    b_ptrs = b_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    b = tl.load(b_ptrs, mask=c_mask, other=0.0)\n",
    "    c_tmp = accumulator + b\n",
    "\n",
    "    c = c_tmp.to(tl.float16)\n",
    "\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd241945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now create a convenience wrapper function that only takes two input tensors,\n",
    "# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n",
    "\n",
    "\n",
    "def matmul_add(a, x, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == x.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert a.shape[0] == b.shape[0], \"Matrix B must have same first dim as Matrix A\"\n",
    "    assert x.shape[1] == b.shape[1], \"Matrix B must have same second dim as Matrix X\"\n",
    "    M, K = a.shape\n",
    "    K, N = x.shape\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_add_kernel[grid](\n",
    "        a, x, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        x.stride(0), x.stride(1),  #\n",
    "        c.stride(0), c.stride(1)\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b23f20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] Output code: \n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] # AOT ID: ['0_inference']\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] import torch\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] import math\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] import random\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] import os\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] import tempfile\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] from math import inf, nan\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] from cmath import nanj\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] from torch._inductor.utils import maybe_profile\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] from torch import device, empty_strided\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] from torch._inductor.codegen.multi_kernel import MultiKernelCall\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] \n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] aten = torch.ops.aten\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] inductor_ops = torch.ops.inductor\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] _quantized = torch.ops._quantized\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] async_compile = AsyncCompile()\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] \n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] \n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] async_compile.wait(globals())\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] del async_compile\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] \n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] def call(args):\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     arg0_1, arg1_1, arg2_1 = args\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     args.clear()\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     assert_size_stride(arg0_1, (512, 512), (512, 1))\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     assert_size_stride(arg1_1, (512, 512), (512, 1))\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     assert_size_stride(arg2_1, (512, 512), (512, 1))\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]         torch.cuda.set_device(0)\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]         buf0 = empty_strided_cuda((512, 512), (512, 1), torch.float16)\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [addmm], Original ATen: [aten.addmm]\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]         extern_kernels.addmm(arg2_1, arg1_1, arg0_1, alpha=1, beta=1, out=buf0)\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]         del arg0_1\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]         del arg1_1\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]         del arg2_1\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     return (buf0, )\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] \n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] \n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     from torch._inductor.utils import print_performance\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     arg0_1 = rand_strided((512, 512), (512, 1), device='cuda:0', dtype=torch.float16)\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     arg1_1 = rand_strided((512, 512), (512, 1), device='cuda:0', dtype=torch.float16)\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     arg2_1 = rand_strided((512, 512), (512, 1), device='cuda:0', dtype=torch.float16)\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1, arg2_1])\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] \n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] \n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] if __name__ == \"__main__\":\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)\n",
      "V0508 03:20:19.146000 5134 torch/_inductor/graph.py:2104] [0/0] [__output_code] \n",
      "V0508 03:20:19.149000 5134 torch/_inductor/graph.py:2115] [0/0] [__output_code] Output code written to: /var/tmp/torchinductor_cloud-user/2b/c2bwsyzh2kjd6iwggc2aijzwcozlf7a5inhcqpsr2aoyjmmsrnjb.py\n",
      "I0508 03:20:19.156000 5134 torch/_inductor/graph.py:2149] [0/0] [__output_code] Output code written to: /var/tmp/torchinductor_cloud-user/2b/c2bwsyzh2kjd6iwggc2aijzwcozlf7a5inhcqpsr2aoyjmmsrnjb.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton_output_with_fp16_inputs=tensor([[ -8.4062,  -5.4258,  15.2031,  ..., -30.8125,   4.2422, -27.4688],\n",
      "        [ 27.8281,   9.6719,  -6.8047,  ..., -11.6875,  -7.0508,  29.4219],\n",
      "        [-14.2578,  14.9375,  19.0000,  ..., -20.6406,  -7.6523,  11.4219],\n",
      "        ...,\n",
      "        [ 38.7188,  16.8750, -25.7969,  ...,  -2.3008,  -4.6016,  42.7812],\n",
      "        [ -6.1562, -18.8125,   3.7383,  ..., -21.7500,  25.5781,  15.2109],\n",
      "        [-14.1641, -19.9688,  -1.2002,  ...,  20.4375, -29.6094, -14.0391]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "torch_output_with_fp16_inputs=tensor([[ -8.4062,  -5.4258,  15.2031,  ..., -30.8125,   4.2422, -27.4688],\n",
      "        [ 27.8281,   9.6719,  -6.8047,  ..., -11.6875,  -7.0508,  29.4219],\n",
      "        [-14.2578,  14.9375,  19.0000,  ..., -20.6406,  -7.6523,  11.4219],\n",
      "        ...,\n",
      "        [ 38.7188,  16.8750, -25.7969,  ...,  -2.3008,  -4.6016,  42.7812],\n",
      "        [ -6.1562, -18.8125,   3.7383,  ..., -21.7500,  25.5781,  15.2109],\n",
      "        [-14.1641, -19.9688,  -1.2002,  ...,  20.4375, -29.6094, -14.0391]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "✅ Triton and Torch match\n",
      "triton_output_with_fp8_inputs=tensor([[-18.9375,  12.4219,   5.5352,  ...,  26.2031,   8.5781, -41.7500],\n",
      "        [ 11.0000,  36.6250,  -7.0664,  ...,  20.6094,  48.3125,  29.5781],\n",
      "        [ 18.5625,  -3.8828, -19.1719,  ...,  -1.1299,  -7.0625,  13.8125],\n",
      "        ...,\n",
      "        [-19.6562, -35.9062, -27.6094,  ..., -26.9531, -25.3438, -10.6016],\n",
      "        [ -3.4258, -10.6250, -24.4062,  ...,  -4.8594,  -2.6582, -15.8828],\n",
      "        [-20.9688,  -4.2617, -34.5625,  ...,  16.3125, -36.0000,  24.9844]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "torch_output_with_fp8_inputs=tensor([[-18.9375,  12.4219,   5.5352,  ...,  26.2031,   8.5781, -41.7500],\n",
      "        [ 11.0000,  36.6250,  -7.0664,  ...,  20.6094,  48.3125,  29.5781],\n",
      "        [ 18.5625,  -3.8828, -19.1719,  ...,  -1.1299,  -7.0625,  13.8125],\n",
      "        ...,\n",
      "        [-19.6562, -35.9062, -27.6094,  ..., -26.9531, -25.3438, -10.6016],\n",
      "        [ -3.4258, -10.6250, -24.4062,  ...,  -4.8594,  -2.6582, -15.8828],\n",
      "        [-20.9688,  -4.2617, -34.5625,  ...,  16.3125, -36.0000,  24.9844]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "# Unit Test\n",
    "# ---------\n",
    "#\n",
    "# We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\n",
    "\n",
    "torch.manual_seed(0)\n",
    "a = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "x = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "b = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "triton_output = matmul_add(a, x, b)\n",
    "torch_compiled = torch.compile(torch.addmm)\n",
    "torch_output = torch_compiled(b, a, x)\n",
    "print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
    "print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
    "# Bigger tolerance for AMD MI200 devices.\n",
    "# MI200 devices use reduced precision fp16 and bf16 and flush input and\n",
    "# output denormal values to zero. Detailed info is at: https://pytorch.org/docs/stable/notes/numerical_accuracy.html#reduced-precision-fp16-and-bf16-gemms-and-convolutions-on-amd-instinct-mi200-devices\n",
    "rtol = 1e-2 if is_hip_mi200() else 0\n",
    "if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=rtol):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ\")\n",
    "\n",
    "TORCH_HAS_FP8 = hasattr(torch, \"float8_e5m2\")\n",
    "if TORCH_HAS_FP8 and is_cuda():\n",
    "    torch.manual_seed(0)\n",
    "    a = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "    x = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "    a = a.to(torch.float8_e5m2)\n",
    "    # pre-transpose x for efficiency.\n",
    "    x = x.T\n",
    "    x = x.to(torch.float8_e5m2)\n",
    "    b = b.to(torch.float8_e5m2)\n",
    "    triton_output = matmul_add(a, x, b)\n",
    "    torch_output = torch.addmm(b.to(torch.float16), a.to(torch.float16), x.to(torch.float16))\n",
    "    print(f\"triton_output_with_fp8_inputs={triton_output}\")\n",
    "    print(f\"torch_output_with_fp8_inputs={torch_output}\")\n",
    "    if torch.allclose(triton_output, torch_output, atol=0.125, rtol=0):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "# ---------\n",
    "#\n",
    "# Square Matrix Performance\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# We can now compare the performance of our kernel against that of cuBLAS or rocBLAS. Here we focus on square matrices,\n",
    "# but feel free to arrange this script as you wish to benchmark any other matrix shape.\n",
    "\n",
    "ref_lib = 'cuBLAS' if is_cuda() else 'rocBLAS'\n",
    "\n",
    "configs = []\n",
    "for fp8_inputs in [False, True]:\n",
    "    if fp8_inputs and (not TORCH_HAS_FP8 or not is_cuda()):\n",
    "        continue\n",
    "    configs.append(\n",
    "        triton.testing.Benchmark(\n",
    "            x_names=[\"M\", \"N\", \"K\"],  # Argument names to use as an x-axis for the plot\n",
    "            x_vals=[128 * i for i in range(2, 33)],  # Different possible values for `x_name`\n",
    "            line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "            # Possible values for `line_arg`\n",
    "            # Don't compare to cublas for fp8 cases as torch.matmul doesn't support fp8 at the moment.\n",
    "            line_vals=[\"triton\"] if fp8_inputs else [ref_lib.lower(), \"triton\", \"compile\"],  # Label name for the lines\n",
    "            line_names=[\"Triton\"] if fp8_inputs else [ref_lib, \"Triton\", \"Compile\"],  # Line styles\n",
    "            styles=[(\"green\", \"-\"), (\"blue\", \"-\"), (\"red\", \"-\")],\n",
    "            ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "            plot_name=\"matmul-performance-\" +\n",
    "            (\"fp16\" if not fp8_inputs else \"fp8\"),  # Name for the plot, used also as a file name for saving the plot.\n",
    "            args={\"fp8_inputs\": fp8_inputs},\n",
    "        ))\n",
    "\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, K, provider, fp8_inputs):\n",
    "    a = torch.randn((M, K), device=DEVICE, dtype=torch.float16)\n",
    "    x = torch.randn((K, N), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((M, N), device=DEVICE, dtype=torch.float16)\n",
    "    if TORCH_HAS_FP8 and fp8_inputs:\n",
    "        a = a.to(torch.float8_e5m2)\n",
    "        x = x.T\n",
    "        x = x.to(torch.float8_e5m2)\n",
    "        b = b.to(torch.float8_e5m2)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.addmm(b, a, x), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul_add(a, x, b), quantiles=quantiles)\n",
    "    if provider == 'compile':\n",
    "        addmm_opt = torch.compile(torch.addmm)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: addmm_opt(b, a, x), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31a1fa-078f-469a-8990-0b941ebbe79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50880f4a-decd-42da-a280-d9462f48bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "a = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "x = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "b = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd446c3e-ae5d-4974-aeda-d61f7da5850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the torch path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e71134-bf82-409c-b7f1-bc6f762fb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_output = torch.addmm(b, a, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfc7d1-3223-4646-a811-52ea590d71f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e14be-3d55-4818-b1b0-d21712b390c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c145e2-e0c8-4e55-ad83-bd5780a992b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the triton path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed545c-25eb-403b-9c85-393aa48db3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_output = matmul_add(a, x, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0aa2ed-64aa-4fc5-af9a-c9424c40f5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a437cc-5b8a-421d-8780-d9af24b4c81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16f525-c7cf-4ee5-98ae-98df7e969ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfbbc2-4af8-4f46-bdd6-a12706634481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_model_torch(a, x, b, length) :\n",
    "    output = torch.addmm(b, a, x)\n",
    "    for i in range(length-1):\n",
    "        output = torch.addmm(output, a, x)\n",
    "\n",
    "    return output\n",
    "\n",
    "def pseudo_model_triton(a, x, b, length):\n",
    "    output = matmul_add(a, x, b)\n",
    "    for i in range(length-1):\n",
    "        output = matmul_add(a, x, output)\n",
    "\n",
    "    return output\n",
    "\n",
    "pseudo_model_compiled = torch.compile(pseudo_model_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97303c50-e01e-445d-9db9-4adc1e18a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "# ---------\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, K, provider, fp8_inputs):\n",
    "    a = torch.randn((M, K), device=DEVICE, dtype=torch.float16)\n",
    "    x = torch.randn((K, N), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((M, N), device=DEVICE, dtype=torch.float16)\n",
    "    if TORCH_HAS_FP8 and fp8_inputs:\n",
    "        a = a.to(torch.float8_e5m2)\n",
    "        x = x.T\n",
    "        x = x.to(torch.float8_e5m2)\n",
    "        b = b.to(torch.float8_e5m2)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    addmm_opt = torch.compile(torch.addmm)\n",
    "    num_ops = 10\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_torch(a, x, b, num_ops), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_triton(a, x, b, num_ops), quantiles=quantiles)\n",
    "    if provider == 'compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_compiled(a, x, b, num_ops), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * num_ops * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa7308-5ddc-4f4e-9e31-4404ae59c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_model_torch_10(a, x, b) :\n",
    "    output = torch.addmm(b, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "\n",
    "    return output\n",
    "\n",
    "def pseudo_model_triton_10(a, x, b):\n",
    "    output = matmul_add(a, x, b)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "pseudo_model_10_compiled = torch.compile(pseudo_model_torch_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de391341-b953-464f-af7a-f29af34e1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "# ---------\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, K, provider, fp8_inputs):\n",
    "    a = torch.randn((M, K), device=DEVICE, dtype=torch.float16)\n",
    "    x = torch.randn((K, N), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((M, N), device=DEVICE, dtype=torch.float16)\n",
    "    if TORCH_HAS_FP8 and fp8_inputs:\n",
    "        a = a.to(torch.float8_e5m2)\n",
    "        x = x.T\n",
    "        x = x.to(torch.float8_e5m2)\n",
    "        b = b.to(torch.float8_e5m2)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    addmm_opt = torch.compile(torch.addmm)\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_torch_10(a, x, b), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_triton_10(a, x, b), quantiles=quantiles)\n",
    "    if provider == 'compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_10_compiled(a, x, b), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 10 * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa47393-81f8-474f-9e62-c4388761e251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
