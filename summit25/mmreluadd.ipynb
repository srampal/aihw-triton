{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579dde73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMatrix Multiplication\\n=====================\\nIn this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\\nperformance on par with cuBLAS or rocBLAS.\\n\\nYou will specifically learn about:\\n\\n* Block-level matrix multiplications.\\n\\n* Multi-dimensional pointer arithmetic.\\n\\n* Program re-ordering for improved L2 cache hit rate.\\n\\n* Automatic performance tuning.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Matrix Multiplication\n",
    "=====================\n",
    "In this tutorial, you will write a very short high-performance FP16 matrix multiplication kernel that achieves\n",
    "performance on par with cuBLAS or rocBLAS.\n",
    "\n",
    "You will specifically learn about:\n",
    "\n",
    "* Block-level matrix multiplications.\n",
    "\n",
    "* Multi-dimensional pointer arithmetic.\n",
    "\n",
    "* Program re-ordering for improved L2 cache hit rate.\n",
    "\n",
    "* Automatic performance tuning.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9578b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motivations\n",
    "# -----------\n",
    "#\n",
    "# Matrix multiplications are a key building block of most modern high-performance computing systems.\n",
    "# They are notoriously hard to optimize, hence their implementation is generally done by\n",
    "# hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n",
    "# Unfortunately, these libraries are often proprietary and cannot be easily customized\n",
    "# to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n",
    "# In this tutorial, you will learn how to implement efficient matrix multiplications by\n",
    "# yourself with Triton, in a way that is easy to customize and extend.\n",
    "#\n",
    "# Roughly speaking, the kernel that we will write will implement the following blocked\n",
    "# algorithm to multiply a (M, K) by a (K, N) matrix:\n",
    "#\n",
    "#  .. code-block:: python\n",
    "#\n",
    "#    # Do in parallel\n",
    "#    for m in range(0, M, BLOCK_SIZE_M):\n",
    "#      # Do in parallel\n",
    "#      for n in range(0, N, BLOCK_SIZE_N):\n",
    "#        acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\n",
    "#        for k in range(0, K, BLOCK_SIZE_K):\n",
    "#          a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n",
    "#          x = X[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n",
    "#          acc += dot(a, x)\n",
    "#        C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\n",
    "#\n",
    "# where each iteration of the doubly-nested for-loop is performed by a dedicated Triton program instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce83832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Kernel\n",
    "# --------------\n",
    "#\n",
    "# The above algorithm is, actually, fairly straightforward to implement in Triton.\n",
    "# The main difficulty comes from the computation of the memory locations at which blocks\n",
    "# of :code:`A` and :code:`X` must be read in the inner loop. For that, we need\n",
    "# multi-dimensional pointer arithmetic.\n",
    "#\n",
    "# Pointer Arithmetic\n",
    "# ~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# For a row-major 2D tensor :code:`X`, the memory location of :code:`X[i, j]` is given\n",
    "# by :code:`&X[i, j] = X + i*stride_xi + j*stride_xj`.\n",
    "# Therefore, blocks of pointers for :code:`A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K]` and\n",
    "# :code:`X[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]` can be defined in pseudo-code as:\n",
    "#\n",
    "#  .. code-block:: python\n",
    "#\n",
    "#    &A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] =  a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1);\n",
    "#    &X[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] =  x_ptr + (k : k+BLOCK_SIZE_K)[:, None]*X.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*X.stride(1);\n",
    "#\n",
    "# Which means that pointers for blocks of A and X can be initialized (i.e., :code:`k=0`) in Triton as the following\n",
    "# code. Also note that we need an extra modulo to handle the case where :code:`M` is not a multiple of\n",
    "# :code:`BLOCK_SIZE_M` or :code:`N` is not a multiple of :code:`BLOCK_SIZE_N`, in which case we can pad the data with\n",
    "# some useless values, which will not contribute to the results. For the :code:`K` dimension, we will handle that later\n",
    "# using masking load semantics.\n",
    "#\n",
    "#  .. code-block:: python\n",
    "#\n",
    "#    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "#    offs_xn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "#    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "#    a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k [None, :]*stride_ak)\n",
    "#    x_ptrs = x_ptr + (offs_k [:, None]*stride_xk + offs_xn[None, :]*stride_xn)\n",
    "#\n",
    "# And then updated in the inner loop as follows:\n",
    "#\n",
    "#  .. code-block:: python\n",
    "#\n",
    "#    a_ptrs += BLOCK_SIZE_K * stride_ak;\n",
    "#    x_ptrs += BLOCK_SIZE_K * stride_xk;\n",
    "#\n",
    "#\n",
    "# L2 Cache Optimizations\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# As mentioned above, each program instance computes a :code:`[BLOCK_SIZE_M, BLOCK_SIZE_N]`\n",
    "# block of :code:`C`.\n",
    "# It is important to remember that the order in which these blocks are computed does\n",
    "# matter, since it affects the L2 cache hit rate of our program, and unfortunately, a\n",
    "# simple row-major ordering\n",
    "#\n",
    "#  .. code-block:: Python\n",
    "#\n",
    "#    pid = tl.program_id(axis=0)\n",
    "#    grid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "#    pid_m = pid // grid_n\n",
    "#    pid_n = pid % grid_n\n",
    "#\n",
    "# is just not going to cut it.\n",
    "#\n",
    "# One possible solution is to launch blocks in an order that promotes data reuse.\n",
    "# This can be done by 'super-grouping' blocks in groups of :code:`GROUP_M` rows before\n",
    "# switching to the next column:\n",
    "#\n",
    "#  .. code-block:: python\n",
    "#\n",
    "#    # Program ID\n",
    "#    pid = tl.program_id(axis=0)\n",
    "#    # Number of program ids along the M axis\n",
    "#    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "#    # Number of programs ids along the N axis\n",
    "#    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "#    # Number of programs in group\n",
    "#    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "#    # Id of the group this program is in\n",
    "#    group_id = pid // num_pid_in_group\n",
    "#    # Row-id of the first program in the group\n",
    "#    first_pid_m = group_id * GROUP_SIZE_M\n",
    "#    # If `num_pid_m` isn't divisible by `GROUP_SIZE_M`, the last group is smaller\n",
    "#    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "#    # *Within groups*, programs are ordered in a column-major order\n",
    "#    # Row-id of the program in the *launch grid*\n",
    "#    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "#    # Col-id of the program in the *launch grid*\n",
    "#    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "#\n",
    "# For example, in the following matmul where each matrix is 9 blocks by 9 blocks,\n",
    "# we can see that if we compute the output in row-major ordering, we need to load 90\n",
    "# blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped\n",
    "# ordering, we only need to load 54 blocks.\n",
    "#\n",
    "#   .. image:: grouped_vs_row_major_ordering.png\n",
    "#\n",
    "# In practice, this can improve the performance of our matrix multiplication kernel by\n",
    "# more than 10\\% on some hardware architecture (e.g., 220 to 245 TFLOPS on A100).\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d387427e-fb2b-42de-99ca-d401c4ab0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build your own triton\n",
    "#%env MAX_JOBS=6\n",
    "#!cd triton && pip install ./python && cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8bd9bd6-2abc-4727-9b6b-924e73cba314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: triton\n",
      "Version: 3.3.0\n",
      "Summary: A language and compiler for custom Deep Learning operations\n",
      "Home-page: https://github.com/triton-lang/triton/\n",
      "Author: Philippe Tillet\n",
      "Author-email: phil@openai.com\n",
      "License: \n",
      "Location: /workspace/lib/python3.12/site-packages\n",
      "Requires: setuptools\n",
      "Required-by: torch\n"
     ]
    }
   ],
   "source": [
    "!pip show triton\n",
    "#!pip show pytorch-triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63abf347",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"TORCH_LOGS\"] = \"output_code\"\n",
    "\n",
    "# Final Result\n",
    "# ------------\n",
    "\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# triton 3.2\n",
    "#DEVICE = triton.runtime.driver.active.get_current_target().backend\n",
    "# triton 3.3\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "\n",
    "\n",
    "def is_hip_mi200():\n",
    "    target = triton.runtime.driver.active.get_current_target()\n",
    "    return target.backend == 'hip' and target.arch == 'gfx90a'\n",
    "\n",
    "\n",
    "def get_cuda_autotune_config():\n",
    "    return [\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        # Good config for fp8 inputs.\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_hip_autotune_config():\n",
    "    return [\n",
    "        triton.Config(\n",
    "            {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2},\n",
    "            num_warps=4, num_stages=2),\n",
    "        triton.Config(\n",
    "            {'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 4, 'waves_per_eu': 2},\n",
    "            num_warps=8, num_stages=2),\n",
    "        triton.Config(\n",
    "            {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2},\n",
    "            num_warps=8, num_stages=2),\n",
    "        triton.Config(\n",
    "            {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8, 'waves_per_eu': 3},\n",
    "            num_warps=4, num_stages=2),\n",
    "        triton.Config(\n",
    "            {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 1, 'waves_per_eu': 8},\n",
    "            num_warps=4, num_stages=2),\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_autotune_config():\n",
    "    if is_cuda():\n",
    "        return get_cuda_autotune_config()\n",
    "    else:\n",
    "        return get_hip_autotune_config()\n",
    "\n",
    "\n",
    "# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n",
    "#   - A list of `triton.Config` objects that define different configurations of\n",
    "#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n",
    "#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n",
    "#       provided configs\n",
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_relu_add_kernel(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, x_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_xk, stride_xn,  #\n",
    "        stride_cm, stride_cn,  # matrix b has same stride as c\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,\n",
    "        ACTIVATION: tl.constexpr  #\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmuladd C = (A x X) + B.\n",
    "    A has shape (M, K), X has shape (K, N) and B and C have shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and X.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `x_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetic` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_xn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    x_ptrs = x_ptr + (offs_k[:, None] * stride_xk + offs_xn[None, :] * stride_xn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and X, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        x = tl.load(x_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator = tl.dot(a, x, accumulator)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        x_ptrs += BLOCK_SIZE_K * stride_xk\n",
    "\n",
    "    if ACTIVATION == \"leaky_relu\":\n",
    "        accumulator = leaky_relu(accumulator)\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    b_ptrs = b_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    b = tl.load(b_ptrs, mask=c_mask, other=0.0)\n",
    "    c_tmp = accumulator + b\n",
    "\n",
    "    c = c_tmp.to(tl.float16)\n",
    "\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `matmul_kernel`.\n",
    "@triton.jit\n",
    "def leaky_relu(x):\n",
    "    return tl.where(x >= 0, x, 0.01 * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd241945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now create a convenience wrapper function that only takes two input tensors,\n",
    "# and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel.\n",
    "\n",
    "\n",
    "def matmul_relu_add(a, x, b, activation=\"\"):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == x.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert a.shape[0] == b.shape[0], \"Matrix B must have same first dim as Matrix A\"\n",
    "    assert x.shape[1] == b.shape[1], \"Matrix B must have same second dim as Matrix X\"\n",
    "    M, K = a.shape\n",
    "    K, N = x.shape\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_relu_add_kernel[grid](\n",
    "        a, x, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        x.stride(0), x.stride(1),  #\n",
    "        c.stride(0), c.stride(1),\n",
    "        ACTIVATION=activation  #\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff199d22-94f1-42f4-a135-9861da023dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_torch_matmul_relu_add(a, x, b):\n",
    "    c = torch.matmul(a, x)\n",
    "    c = F.leaky_relu(c, negative_slope=0.01)\n",
    "    c = c + b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b778f45c-3a0c-4563-97e7-f632e787ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def my_torch_matmul_relu_add_compiled(a, x, b):\n",
    "    c = torch.matmul(a, x)\n",
    "    c = F.leaky_relu(c, negative_slope=0.01)\n",
    "    c = c + b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b23f20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton_output_with_fp16_inputs=tensor([[ 2.4355, -0.7632, 15.2031,  ..., -2.7012,  4.2422, -1.3047],\n",
      "        [27.8281,  9.6719, -1.4307,  ..., -0.5039,  1.3936, 29.4219],\n",
      "        [-1.1318, 14.9375, 19.0000,  ...,  0.9014,  0.8994, 11.4219],\n",
      "        ...,\n",
      "        [38.7188, 16.8750, -0.4492,  ...,  0.4314, -1.3887, 42.7812],\n",
      "        [-0.0944, -2.1523,  3.7383,  ..., -0.9268, 25.5781, 15.2109],\n",
      "        [ 2.7500, -1.1572, -0.8208,  ..., 20.4375,  0.3110, -0.9468]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "torch_output_with_fp16_inputs=tensor([[ 2.4355, -0.7637, 15.2031,  ..., -2.7031,  4.2422, -1.3047],\n",
      "        [27.8281,  9.6641, -1.4307,  ..., -0.5039,  1.3936, 29.4219],\n",
      "        [-1.1318, 14.9375, 19.0000,  ...,  0.9014,  0.8994, 11.4219],\n",
      "        ...,\n",
      "        [38.7188, 16.8750, -0.4492,  ...,  0.4314, -1.3887, 42.7812],\n",
      "        [-0.0944, -2.1523,  3.7383,  ..., -0.9268, 25.5781, 15.2188],\n",
      "        [ 2.7500, -1.1572, -0.8208,  ..., 20.4375,  0.3110, -0.9468]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "✅ Triton and Torch match\n",
      "triton_output_with_fp8_inputs=tensor([[ 2.2852e+00,  1.2422e+01,  5.5352e+00,  ...,  2.6203e+01,\n",
      "          8.5781e+00, -1.4072e+00],\n",
      "        [ 1.1000e+01,  3.6625e+01, -1.5557e+00,  ...,  2.0609e+01,\n",
      "          4.8312e+01,  2.9578e+01],\n",
      "        [ 1.8562e+01, -9.0527e-01,  6.7432e-01,  ...,  9.7852e-01,\n",
      "          9.1943e-01,  1.3812e+01],\n",
      "        ...,\n",
      "        [-1.6816e+00, -2.0918e+00, -4.6167e-01,  ...,  1.6357e-01,\n",
      "         -1.4912e+00,  1.6260e+00],\n",
      "        [-6.5186e-02, -2.0859e+00, -9.8633e-01,  ..., -7.9102e-01,\n",
      "          8.3984e-01,  2.6779e-02],\n",
      "        [ 2.7598e+00, -1.0322e+00, -1.2119e+00,  ...,  1.6312e+01,\n",
      "          2.5879e-01,  2.4984e+01]], device='cuda:0', dtype=torch.float16)\n",
      "torch_output_with_fp8_inputs=tensor([[ 2.2852e+00,  1.2422e+01,  5.5352e+00,  ...,  2.6203e+01,\n",
      "          8.5781e+00, -1.4072e+00],\n",
      "        [ 1.1000e+01,  3.6625e+01, -1.5557e+00,  ...,  2.0609e+01,\n",
      "          4.8312e+01,  2.9578e+01],\n",
      "        [ 1.8562e+01, -9.0527e-01,  6.7480e-01,  ...,  9.7852e-01,\n",
      "          9.1943e-01,  1.3812e+01],\n",
      "        ...,\n",
      "        [-1.6816e+00, -2.0918e+00, -4.6167e-01,  ...,  1.6357e-01,\n",
      "         -1.4912e+00,  1.6270e+00],\n",
      "        [-6.5186e-02, -2.0859e+00, -9.8633e-01,  ..., -7.9102e-01,\n",
      "          8.3984e-01,  2.6733e-02],\n",
      "        [ 2.7598e+00, -1.0322e+00, -1.2119e+00,  ...,  1.6312e+01,\n",
      "          2.5879e-01,  2.4984e+01]], device='cuda:0', dtype=torch.float16)\n",
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "# Unit Test\n",
    "# ---------\n",
    "#\n",
    "# We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS).\n",
    "\n",
    "torch.manual_seed(0)\n",
    "a = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "x = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "b = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "triton_output = matmul_relu_add(a, x, b, \"leaky_relu\")\n",
    "torch_output = my_torch_matmul_relu_add(a, x, b)\n",
    "print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
    "print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
    "# Bigger tolerance for AMD MI200 devices.\n",
    "# MI200 devices use reduced precision fp16 and bf16 and flush input and\n",
    "# output denormal values to zero. Detailed info is at: https://pytorch.org/docs/stable/notes/numerical_accuracy.html#reduced-precision-fp16-and-bf16-gemms-and-convolutions-on-amd-instinct-mi200-devices\n",
    "rtol = 1e-2 if is_hip_mi200() else 0\n",
    "if torch.allclose(triton_output, torch_output, atol=0.125, rtol=rtol):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ\")\n",
    "\n",
    "TORCH_HAS_FP8 = hasattr(torch, \"float8_e5m2\")\n",
    "if TORCH_HAS_FP8 and is_cuda():\n",
    "    torch.manual_seed(0)\n",
    "    a = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "    x = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "    a = a.to(torch.float8_e5m2)\n",
    "    # pre-transpose x for efficiency.\n",
    "    x = x.T\n",
    "    x = x.to(torch.float8_e5m2)\n",
    "    b = b.to(torch.float8_e5m2)\n",
    "    triton_output = matmul_relu_add(a, x, b, \"leaky_relu\")\n",
    "    torch_output = my_torch_matmul_relu_add(a.to(torch.float16), x.to(torch.float16), b.to(torch.float16))\n",
    "    print(f\"triton_output_with_fp8_inputs={triton_output}\")\n",
    "    print(f\"torch_output_with_fp8_inputs={torch_output}\")\n",
    "    if torch.allclose(triton_output, torch_output, atol=0.125, rtol=0):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "# ---------\n",
    "#\n",
    "# Square Matrix Performance\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#\n",
    "# We can now compare the performance of our kernel against that of cuBLAS or rocBLAS. Here we focus on square matrices,\n",
    "# but feel free to arrange this script as you wish to benchmark any other matrix shape.\n",
    "\n",
    "ref_lib = 'cuBLAS' if is_cuda() else 'rocBLAS'\n",
    "\n",
    "configs = []\n",
    "for fp8_inputs in [False, True]:\n",
    "    if fp8_inputs and (not TORCH_HAS_FP8 or not is_cuda()):\n",
    "        continue\n",
    "    configs.append(\n",
    "        triton.testing.Benchmark(\n",
    "            x_names=[\"M\", \"N\", \"K\"],  # Argument names to use as an x-axis for the plot\n",
    "            x_vals=[128 * i for i in range(2, 33)],  # Different possible values for `x_name`\n",
    "            line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "            # Possible values for `line_arg`\n",
    "            # Don't compare to cublas for fp8 cases as torch.matmul doesn't support fp8 at the moment.\n",
    "            line_vals=[\"triton\"] if fp8_inputs else [ref_lib.lower(), \"triton\", \"compile\"],  # Label name for the lines\n",
    "            line_names=[\"Triton\"] if fp8_inputs else [ref_lib, \"Triton\", \"Compile\"],  # Line styles\n",
    "            styles=[(\"green\", \"-\"), (\"blue\", \"-\"), (\"red\", \"-\")],\n",
    "            ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "            plot_name=\"matmul-performance-\" +\n",
    "            (\"fp16\" if not fp8_inputs else \"fp8\"),  # Name for the plot, used also as a file name for saving the plot.\n",
    "            args={\"fp8_inputs\": fp8_inputs},\n",
    "        ))\n",
    "\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, K, provider, fp8_inputs):\n",
    "    a = torch.randn((M, K), device=DEVICE, dtype=torch.float16)\n",
    "    x = torch.randn((K, N), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((M, N), device=DEVICE, dtype=torch.float16)\n",
    "    if TORCH_HAS_FP8 and fp8_inputs:\n",
    "        a = a.to(torch.float8_e5m2)\n",
    "        x = x.T\n",
    "        x = x.to(torch.float8_e5m2)\n",
    "        b = b.to(torch.float8_e5m2)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: my_torch_matmul_relu_add(a, x, b), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul_relu_add(a, x, b, \"leaky_relu\"), quantiles=quantiles)\n",
    "    if provider == 'compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: my_torch_matmul_relu_add_compiled(a, x, b), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31a1fa-078f-469a-8990-0b941ebbe79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50880f4a-decd-42da-a280-d9462f48bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "a = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "x = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "b = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd446c3e-ae5d-4974-aeda-d61f7da5850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the torch path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e71134-bf82-409c-b7f1-bc6f762fb151",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_output = torch.addmm(b, a, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dfc7d1-3223-4646-a811-52ea590d71f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e14be-3d55-4818-b1b0-d21712b390c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c145e2-e0c8-4e55-ad83-bd5780a992b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile the triton path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed545c-25eb-403b-9c85-393aa48db3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_output = matmul_add(a, x, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0aa2ed-64aa-4fc5-af9a-c9424c40f5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a437cc-5b8a-421d-8780-d9af24b4c81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16f525-c7cf-4ee5-98ae-98df7e969ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edfbbc2-4af8-4f46-bdd6-a12706634481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_model_torch(a, x, b, length) :\n",
    "    output = torch.addmm(b, a, x)\n",
    "    for i in range(length-1):\n",
    "        output = torch.addmm(output, a, x)\n",
    "\n",
    "    return output\n",
    "\n",
    "def pseudo_model_triton(a, x, b, length):\n",
    "    output = matmul_add(a, x, b)\n",
    "    for i in range(length-1):\n",
    "        output = matmul_add(a, x, output)\n",
    "\n",
    "    return output\n",
    "\n",
    "pseudo_model_compiled = torch.compile(pseudo_model_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97303c50-e01e-445d-9db9-4adc1e18a47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "# ---------\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, K, provider, fp8_inputs):\n",
    "    a = torch.randn((M, K), device=DEVICE, dtype=torch.float16)\n",
    "    x = torch.randn((K, N), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((M, N), device=DEVICE, dtype=torch.float16)\n",
    "    if TORCH_HAS_FP8 and fp8_inputs:\n",
    "        a = a.to(torch.float8_e5m2)\n",
    "        x = x.T\n",
    "        x = x.to(torch.float8_e5m2)\n",
    "        b = b.to(torch.float8_e5m2)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    addmm_opt = torch.compile(torch.addmm)\n",
    "    num_ops = 10\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_torch(a, x, b, num_ops), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_triton(a, x, b, num_ops), quantiles=quantiles)\n",
    "    if provider == 'compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_compiled(a, x, b, num_ops), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * num_ops * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa7308-5ddc-4f4e-9e31-4404ae59c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_model_torch_10(a, x, b) :\n",
    "    output = torch.addmm(b, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "    output = torch.addmm(output, a, x)\n",
    "\n",
    "    return output\n",
    "\n",
    "def pseudo_model_triton_10(a, x, b):\n",
    "    output = matmul_add(a, x, b)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    output = matmul_add(a, x, output)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "pseudo_model_10_compiled = torch.compile(pseudo_model_torch_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de391341-b953-464f-af7a-f29af34e1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "# ---------\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, K, provider, fp8_inputs):\n",
    "    a = torch.randn((M, K), device=DEVICE, dtype=torch.float16)\n",
    "    x = torch.randn((K, N), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((M, N), device=DEVICE, dtype=torch.float16)\n",
    "    if TORCH_HAS_FP8 and fp8_inputs:\n",
    "        a = a.to(torch.float8_e5m2)\n",
    "        x = x.T\n",
    "        x = x.to(torch.float8_e5m2)\n",
    "        b = b.to(torch.float8_e5m2)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    addmm_opt = torch.compile(torch.addmm)\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_torch_10(a, x, b), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_triton_10(a, x, b), quantiles=quantiles)\n",
    "    if provider == 'compile':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: pseudo_model_10_compiled(a, x, b), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 10 * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa47393-81f8-474f-9e62-c4388761e251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
